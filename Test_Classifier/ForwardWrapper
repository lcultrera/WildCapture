def my_forward_wrapper(attn_obj):
    def my_forward(x):
        B, N, C = x.shape
        qkv = attn_obj.qkv(x).reshape(B, N, 3, attn_obj.num_heads, C // attn_obj.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)

        attn = (q @ k.transpose(-2, -1)) * attn_obj.scale
        attn = attn.softmax(dim=-1)
        attn = attn_obj.attn_drop(attn)
        attn_obj.attn_map = attn
        attn_obj.cls_attn_map = attn[:, :, 0, 2:]

        x = (attn @ v).transpose(1, 2).reshape(B, N, C)
        x = attn_obj.proj(x)
        x = attn_obj.proj_drop(x)
        return x
    return my_forward



class EnsambleModel(torch.nn.Module):
    def __init__(self):
        super(EnsambleModel, self).__init__()
        self.modelVit = timm.create_model('vit_base_patch16_224.augreg_in21k', pretrained=True, num_classes=7)
        data_config = timm.data.resolve_model_data_config(self.modelVit)
        self.transforms = timm.data.create_transform(**data_config, is_training=True)

        self.modelVit.blocks[-1].attn.forward = my_forward_wrapper(self.modelVit.blocks[-1].attn)

    def forward(self, x):
        torch.cuda.empty_cache()

        #x = torch.squeeze(x)
        #x = Image.fromarray(x.detach().cpu().numpy().astype('uint8'), 'RGB')
        #x = self.transforms(x).unsqueeze(0)
        #print(x.shape)
        #print(type(x))
        d1 = self.modelVit(x)
        #attn_map = self.modelVit.blocks[-1].attn.attn_map
        #attn_map = attn_map[:,-1,:,:].squeeze(0).detach().cpu().numpy()

        #attn_map = attn_map.mean(dim=1).squeeze(0).detach().cpu().numpy()

        cls_weight = self.modelVit.blocks[-1].attn.cls_attn_map
        cls_weight =  cls_weight.mean(dim=1)

        a = torch.empty(cls_weight.shape[0], 1)
        a[:,0] = cls_weight[:,165]
        cls = torch.cat((cls_weight, a.cuda()), 1)
        cls = cls.view(-1, 14, 14, 1).detach().cpu().numpy()
        #cls_resized = F.interpolate(cls(1, 1, 14, 14), (224, 224), mode='bilinear').view( 224, 224, 1)
        cls = np.squeeze(cls,axis=0)
        return d1, cls, attn_map

outputs, cls, att = net(inp)
